<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Heuristic Evaluation | HCI Methods Superbook</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="Heuristic Evaluation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Expert-based usability assessment using established principles" />
<meta property="og:description" content="Expert-based usability assessment using established principles" />
<link rel="canonical" href="http://0.0.0.0:12000/hci-methods-superbook/methods/heuristic-evaluation/" />
<meta property="og:url" content="http://0.0.0.0:12000/hci-methods-superbook/methods/heuristic-evaluation/" />
<meta property="og:site_name" content="HCI Methods Superbook" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-02T07:34:09+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Heuristic Evaluation" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-06-02T07:34:09+00:00","datePublished":"2025-06-02T07:34:09+00:00","description":"Expert-based usability assessment using established principles","headline":"Heuristic Evaluation","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:12000/hci-methods-superbook/methods/heuristic-evaluation/"},"url":"http://0.0.0.0:12000/hci-methods-superbook/methods/heuristic-evaluation/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/hci-methods-superbook/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://0.0.0.0:12000/hci-methods-superbook/feed.xml" title="HCI Methods Superbook" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/hci-methods-superbook/">HCI Methods Superbook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/hci-methods-superbook/">HCI Methods Superbook</a><a class="page-link" href="/hci-methods-superbook/methods/">HCI Evaluation Methods</a><a class="page-link" href="/hci-methods-superbook/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Heuristic Evaluation</h1>
  </header>

  <div class="post-content">
    <h1 id="heuristic-evaluation">Heuristic Evaluation</h1>

<h2 id="overview">Overview</h2>

<p>Heuristic evaluation is a usability inspection method where evaluators examine an interface and judge its compliance with recognized usability principles (heuristics). This expert-based evaluation method can identify usability problems quickly and cost-effectively, making it particularly valuable in iterative design processes.</p>

<p>ðŸ“š <strong><a href="/hci-methods-superbook/studies/heuristic-evaluation/">View Specific Studies Using This Method</a></strong> - Explore real research examples and findings</p>

<h2 id="core-principles">Core Principles</h2>

<h3 id="nielsens-10-usability-heuristics">Nielsenâ€™s 10 Usability Heuristics</h3>
<ol>
  <li><strong>Visibility of system status</strong> - Keep users informed about whatâ€™s happening</li>
  <li><strong>Match between system and real world</strong> - Use familiar language and concepts</li>
  <li><strong>User control and freedom</strong> - Provide undo and redo functionality</li>
  <li><strong>Consistency and standards</strong> - Follow platform conventions</li>
  <li><strong>Error prevention</strong> - Design to prevent problems from occurring</li>
  <li><strong>Recognition rather than recall</strong> - Make objects and actions visible</li>
  <li><strong>Flexibility and efficiency of use</strong> - Accommodate both novice and expert users</li>
  <li><strong>Aesthetic and minimalist design</strong> - Avoid irrelevant information</li>
  <li><strong>Help users recognize, diagnose, and recover from errors</strong> - Clear error messages</li>
  <li><strong>Help and documentation</strong> - Provide searchable, task-focused help</li>
</ol>

<h3 id="wearable-specific-heuristics">Wearable-Specific Heuristics</h3>
<ul>
  <li><strong>Context awareness</strong> - Adapt to userâ€™s current situation and environment</li>
  <li><strong>Minimal attention</strong> - Require minimal cognitive load and visual attention</li>
  <li><strong>Glanceability</strong> - Present information that can be quickly understood</li>
  <li><strong>Physical comfort</strong> - Consider ergonomics and long-term wearability</li>
  <li><strong>Battery efficiency</strong> - Optimize for extended use without frequent charging</li>
</ul>

<h2 id="methodology">Methodology</h2>

<h3 id="preparation-phase">Preparation Phase</h3>
<ol>
  <li><strong>Define scope</strong> - Specify which parts of the interface to evaluate</li>
  <li><strong>Select heuristics</strong> - Choose appropriate set of principles for the domain</li>
  <li><strong>Recruit evaluators</strong> - Typically 3-5 usability experts</li>
  <li><strong>Prepare materials</strong> - Provide interface access and evaluation forms</li>
</ol>

<h3 id="evaluation-phase">Evaluation Phase</h3>
<ol>
  <li><strong>Individual inspection</strong> - Each evaluator examines interface independently</li>
  <li><strong>Multiple passes</strong> - First for general impression, second for detailed analysis</li>
  <li><strong>Problem documentation</strong> - Record issues with specific heuristic violations</li>
  <li><strong>Severity rating</strong> - Assess impact and frequency of each problem</li>
</ol>

<h3 id="consolidation-phase">Consolidation Phase</h3>
<ol>
  <li><strong>Aggregate findings</strong> - Combine results from all evaluators</li>
  <li><strong>Remove duplicates</strong> - Identify overlapping problem reports</li>
  <li><strong>Prioritize issues</strong> - Rank problems by severity and impact</li>
  <li><strong>Generate recommendations</strong> - Provide specific suggestions for improvement</li>
</ol>

<h2 id="severity-rating-scale">Severity Rating Scale</h2>

<h3 id="0---not-a-usability-problem">0 - Not a usability problem</h3>
<p>Issue doesnâ€™t affect usability</p>

<h3 id="1---cosmetic-problem">1 - Cosmetic problem</h3>
<p>Minor issue that doesnâ€™t impede task completion</p>

<h3 id="2---minor-usability-problem">2 - Minor usability problem</h3>
<p>Small impact on usability, low priority fix</p>

<h3 id="3---major-usability-problem">3 - Major usability problem</h3>
<p>Significant impact, important to fix</p>

<h3 id="4---usability-catastrophe">4 - Usability catastrophe</h3>
<p>Critical issue that prevents task completion</p>

<h2 id="applications">Applications</h2>

<h3 id="interface-design">Interface Design</h3>
<ul>
  <li><strong>Early design validation</strong> - Evaluate wireframes and prototypes</li>
  <li><strong>Competitive analysis</strong> - Compare interfaces against established standards</li>
  <li><strong>Design iteration</strong> - Quick feedback for iterative improvement</li>
  <li><strong>Quality assurance</strong> - Final check before release</li>
</ul>

<h3 id="wearable-device-evaluation">Wearable Device Evaluation</h3>
<ul>
  <li><strong>Interaction design</strong> - Assess touch, gesture, and voice interfaces</li>
  <li><strong>Information architecture</strong> - Evaluate navigation and content organization</li>
  <li><strong>Notification design</strong> - Review alert and prompt effectiveness</li>
  <li><strong>Cross-device consistency</strong> - Ensure coherence across device ecosystem</li>
</ul>

<h3 id="mobile-and-web-applications">Mobile and Web Applications</h3>
<ul>
  <li><strong>Responsive design</strong> - Evaluate across different screen sizes</li>
  <li><strong>Accessibility compliance</strong> - Check adherence to accessibility guidelines</li>
  <li><strong>Platform consistency</strong> - Ensure compliance with platform standards</li>
  <li><strong>Feature integration</strong> - Assess how new features fit existing interface</li>
</ul>

<h2 id="advantages-and-limitations">Advantages and Limitations</h2>

<h3 id="advantages">Advantages</h3>
<ol>
  <li><strong>Cost-effective</strong> - Relatively inexpensive compared to user testing</li>
  <li><strong>Fast results</strong> - Can be completed in days rather than weeks</li>
  <li><strong>Early feedback</strong> - Applicable to early design stages</li>
  <li><strong>Expert insight</strong> - Leverages evaluator expertise and experience</li>
  <li><strong>Systematic approach</strong> - Structured method ensures comprehensive coverage</li>
</ol>

<h3 id="limitations">Limitations</h3>
<ol>
  <li><strong>Expert dependency</strong> - Quality depends on evaluator expertise</li>
  <li><strong>Limited user perspective</strong> - May miss real user problems and priorities</li>
  <li><strong>False positives</strong> - May identify problems that donâ€™t affect actual users</li>
  <li><strong>Subjective interpretation</strong> - Different evaluators may reach different conclusions</li>
  <li><strong>Context limitations</strong> - May not capture usage context and environment</li>
</ol>

<h2 id="best-practices">Best Practices</h2>

<h3 id="evaluator-selection">Evaluator Selection</h3>
<ol>
  <li><strong>Domain expertise</strong> - Choose evaluators familiar with the application domain</li>
  <li><strong>Usability knowledge</strong> - Ensure understanding of usability principles</li>
  <li><strong>Diverse perspectives</strong> - Include evaluators with different backgrounds</li>
  <li><strong>Appropriate number</strong> - 3-5 evaluators typically optimal</li>
</ol>

<h3 id="evaluation-process">Evaluation Process</h3>
<ol>
  <li><strong>Independent evaluation</strong> - Prevent evaluator bias and groupthink</li>
  <li><strong>Multiple sessions</strong> - Allow time for thorough examination</li>
  <li><strong>Detailed documentation</strong> - Record specific examples and locations</li>
  <li><strong>Consistent criteria</strong> - Use standardized severity ratings</li>
</ol>

<h3 id="reporting-and-follow-up">Reporting and Follow-up</h3>
<ol>
  <li><strong>Clear problem descriptions</strong> - Specific, actionable problem statements</li>
  <li><strong>Supporting evidence</strong> - Screenshots and examples of violations</li>
  <li><strong>Prioritized recommendations</strong> - Focus on high-impact improvements</li>
  <li><strong>Validation planning</strong> - Consider user testing for critical issues</li>
</ol>

<h2 id="integration-with-other-methods">Integration with Other Methods</h2>

<h3 id="complementary-approaches">Complementary Approaches</h3>
<ul>
  <li><strong>User testing</strong> - Validate heuristic findings with real users</li>
  <li><strong>Analytics review</strong> - Compare expert predictions with usage data</li>
  <li><strong>Accessibility audit</strong> - Combine with specialized accessibility evaluation</li>
  <li><strong>Cognitive walkthroughs</strong> - Add task-based evaluation perspective</li>
</ul>

<h3 id="design-process-integration">Design Process Integration</h3>
<ol>
  <li><strong>Early design</strong> - Evaluate wireframes and early prototypes</li>
  <li><strong>Iterative refinement</strong> - Quick feedback during design iterations</li>
  <li><strong>Pre-launch review</strong> - Final quality check before release</li>
  <li><strong>Post-launch assessment</strong> - Identify improvement opportunities</li>
</ol>

<h2 id="specialized-applications">Specialized Applications</h2>

<h3 id="wearable-devices">Wearable Devices</h3>
<ul>
  <li><strong>Glance interaction</strong> - Evaluate quick information access</li>
  <li><strong>Gesture recognition</strong> - Assess gesture interface usability</li>
  <li><strong>Voice interaction</strong> - Review voice command effectiveness</li>
  <li><strong>Multi-modal interfaces</strong> - Evaluate combined interaction modalities</li>
</ul>

<h3 id="accessibility-evaluation">Accessibility Evaluation</h3>
<ul>
  <li><strong>WCAG compliance</strong> - Check adherence to accessibility guidelines</li>
  <li><strong>Assistive technology</strong> - Evaluate compatibility with screen readers</li>
  <li><strong>Motor accessibility</strong> - Assess usability for users with motor impairments</li>
  <li><strong>Cognitive accessibility</strong> - Review clarity and simplicity</li>
</ul>

<h2 id="sources-and-further-reading">Sources and Further Reading</h2>

<p>This method description is based on research from:</p>
<ul>
  <li>ACM Digital Library publications on usability evaluation</li>
  <li>CHI Conference proceedings on inspection methods</li>
  <li>Nielsen Norman Group usability research</li>
  <li>International standards for usability evaluation</li>
</ul>

<h3 id="key-references">Key References</h3>

<p><strong>Foundational Papers:</strong></p>
<ul>
  <li>Nielsen, J., &amp; Molich, R. (1990). Heuristic evaluation of user interfaces. <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (pp. 249-256).</li>
  <li>Nielsen, J. (1994). Enhancing the explanatory power of usability heuristics. <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (pp. 152-158).</li>
  <li>Nielsen, J. (1994). Usability inspection methods. <em>Conference Companion on Human Factors in Computing Systems</em> (pp. 413-414).</li>
</ul>

<p><strong>Methodological Development:</strong></p>
<ul>
  <li>Cockton, G., &amp; Woolrych, A. (2001). Understanding inspection methods: lessons from an assessment of heuristic evaluation. <em>People and Computers XVâ€”Interaction without Frontiers</em> (pp. 171-191).</li>
  <li>Jeffries, R., Miller, J. R., Wharton, C., &amp; Uyeda, K. (1991). User interface evaluation in the real world: a comparison of four techniques. <em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (pp. 119-124).</li>
  <li>Molich, R., &amp; Nielsen, J. (1990). Improving a human-computer dialogue. <em>Communications of the ACM</em>, 33(3), 338-348.</li>
</ul>

<p><strong>Comparative Studies:</strong></p>
<ul>
  <li>Desurvire, H., Caplan, M., &amp; Toth, J. A. (2004). Using heuristics to evaluate the playability of games. <em>CHIâ€™04 Extended Abstracts on Human Factors in Computing Systems</em> (pp. 1509-1512).</li>
  <li>Law, E. L. C., &amp; Hvannberg, E. T. (2004). Analysis of strategies for improving and estimating the effectiveness of heuristic evaluation. <em>Proceedings of the Third Nordic Conference on Human-Computer Interaction</em> (pp. 241-250).</li>
</ul>

<p><strong>Domain-Specific Applications:</strong></p>
<ul>
  <li>Zhang, J., Johnson, T. R., Patel, V. L., Paige, D. L., &amp; Kubose, T. (2003). Using usability heuristics to evaluate patient safety of medical devices. <em>Journal of Biomedical Informatics</em>, 36(1-2), 23-30.</li>
  <li>Bertini, E., Catarci, T., Dix, A., Gabrielli, S., Kimani, S., &amp; Santucci, G. (2009). Appropriating heuristic evaluation for mobile computing. <em>International Journal of Mobile Human Computer Interaction</em>, 1(1), 20-41.</li>
</ul>

<p><strong>Recent Developments:</strong></p>
<ul>
  <li>QuiÃ±ones, D., &amp; Rusu, C. (2017). How to develop usability heuristics: A systematic literature review. <em>Computer Standards &amp; Interfaces</em>, 53, 89-122.</li>
  <li>Hermawati, S., &amp; Lawson, G. (2016). Establishing usability heuristics for heuristics evaluation in a specific domain: Is there a consensus? <em>Applied Ergonomics</em>, 56, 34-51.</li>
</ul>

<p><strong>IEEE and ACM Research:</strong></p>
<ul>
  <li>Tan, W., Liu, D., &amp; Bishu, R. (2009). Web evaluation: Heuristic evaluation vs. user testing. <em>International Journal of Industrial Ergonomics</em>, 39(4), 621-627.</li>
  <li>Chattratichart, J., &amp; Brodie, J. (2004). Applying user interface design principles to the design of an augmented reality system interface. <em>Proceedings of the 3rd IEEE/ACM International Symposium on Mixed and Augmented Reality</em> (pp. 218-226).</li>
</ul>

<hr />

<table>
  <tbody>
    <tr>
      <td>*Last updated: June 2025</td>
      <td>Based on current research from ACM Digital Library and IEEE Xplore*</td>
    </tr>
  </tbody>
</table>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/hci-methods-superbook/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">HCI Methods Superbook</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">HCI Methods Superbook</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A comprehensive collection and categorization of Human-Computer Interaction evaluation methods, with emphasis on wearable devices</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
